# MediChat-AI: Healthcare LLM Assistant

[![Streamlit Demo](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://ai-health-chatbot.streamlit.app/)
![Python Version](https://img.shields.io/badge/python-3.9%2B-blue)

An AI-powered healthcare chatbot combining **Llama 3.1** for generation and **DistilBERT** for intent classification, deployable via Streamlit/Flask or AWS infrastructure.

## Demo
![Result](Images-MCB/medichat-demo.gif)
![Result](Images-MCB/mcb1.png)

## Features
- ðŸ©º **Medical-Specific Responses**: Context-aware healthcare answers
- ðŸ§  **Dual-Model Pipeline**:
  - Intent classification (DistilBERT)
  - Answer generation (T5/Llama-3)
- ðŸš€ **Flexible Deployment**:
  - Streamlit prototype
  - Flask REST API
  - AWS EC2 + S3 (production)

## Quick Start
```bash
git clone https://github.com/yourusername/MediChat-AI.git
cd MediChat-AI
pip install -r requirements.txt

# Set API key (get from https://together.ai)
echo "TOGETHER_API_KEY=your_key_here" > .env

# Run Streamlit demo
streamlit run app.py
```

## Project Structure
```
â”œâ”€â”€ app.py                # Streamlit interface
â”œâ”€â”€ chatbot_webapp.py     # Flask API
â”œâ”€â”€ llm.py                # Together.ai LLM wrapper
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ /models               # For local model storage
â”‚   â”œâ”€â”€ intent_classifier_minimal
â”‚   â””â”€â”€ generator_minimal
```

## Deployment Options

### 1. Local Development
```bash
streamlit run app.py  # Demo on http://localhost:8501
python chatbot_webapp.py  # Flask API on :5000
```

### 2. AWS Production
```mermaid
graph LR
    A[EC2 Instance] -->|Load| B(S3 Models)
    A -->|Serve| C(API Gateway)
    D[User] -->|HTTPS| C
```
**Requirements**:
- EC2 instance (GPU recommended)
- S3 bucket for models
- IAM roles with S3 access

*âš ï¸ AWS configuration not included - set your own credentials*

## Architecture
```mermaid
sequenceDiagram
    User->>UI: Ask question
    UI->>Intent Classifier: Get focus area
    Intent Classifier->>Answer Engine: Structured prompt
    Answer Engine->>LLM: Augmented request
    LLM->>User: Verified response
```

## Contributing
1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add feature'`)
4. Push (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License
Apache 2.0 - See [LICENSE](LICENSE)
```
